# llama-in-the-midle
99% vibe coded proxy of llama.cpp that (for /completion)  connects to several llama.cpp servers and concatenate outputs from one to prompt before calling another one.
